{"cells":[{"cell_type":"markdown","source":["# TheSparkSession"],"metadata":{}},{"cell_type":"code","source":["# Show the details of the SparkSession\nspark"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://10.172.231.56:48672\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v2.4.3</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "]}}],"execution_count":2},{"cell_type":"code","source":["# Create a DataFrame and perform an action\nmyRange = spark.range(1000).toDF(\"number\")\nmyRange.show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+\nnumber|\n+------+\n     0|\n     1|\n     2|\n     3|\n     4|\n+------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["# Create a DataFrame and perform a transformation and an action\ndivisBy2 = myRange.where(\"number % 2 = 0\")\ndivisBy2.show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+\nnumber|\n+------+\n     0|\n     2|\n     4|\n     6|\n     8|\n+------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["# An End-to-End Example"],"metadata":{}},{"cell_type":"code","source":["# Load the data (a transformation)\nflightData2015 = spark\\\n  .read\\\n  .option(\"inferSchema\", \"true\")\\\n  .option(\"header\", \"true\")\\\n  .csv(\"/databricks-datasets/definitive-guide/data/flight-data/csv/2015-summary.csv\")\n\n# Show some rows (an action)\nflightData2015.take(3)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">17</span><span class=\"ansired\">]: </span>[Row(DEST_COUNTRY_NAME=&apos;United States&apos;, ORIGIN_COUNTRY_NAME=&apos;Romania&apos;, count=15),\n Row(DEST_COUNTRY_NAME=&apos;United States&apos;, ORIGIN_COUNTRY_NAME=&apos;Croatia&apos;, count=1),\n Row(DEST_COUNTRY_NAME=&apos;United States&apos;, ORIGIN_COUNTRY_NAME=&apos;Ireland&apos;, count=344)]</div>"]}}],"execution_count":6},{"cell_type":"code","source":["# Sort the rows (a transformation) and look at the explain plan\nflightData2015.sort(\"count\").explain()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\nSort [count#170 ASC NULLS FIRST], true, 0\n+- Exchange rangepartitioning(count#170 ASC NULLS FIRST, 200)\n   +- *(1) FileScan csv [DEST_COUNTRY_NAME#168,ORIGIN_COUNTRY_NAME#169,count#170] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/databricks-datasets/definitive-guide/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int&gt;\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["# Create a table\nflightData2015.createOrReplaceTempView(\"flight_data_2015\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["# Execute a command in the sql way\nsqlWay = spark.sql(\"\"\"\nSELECT DEST_COUNTRY_NAME, count(1)\nFROM flight_data_2015\nGROUP BY DEST_COUNTRY_NAME\n\"\"\")\nsqlWay.explain()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\n*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#168], functions=[finalmerge_count(merge count#185L) AS count(1)#180L])\n+- Exchange hashpartitioning(DEST_COUNTRY_NAME#168, 200)\n   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#168], functions=[partial_count(1) AS count#185L])\n      +- *(1) FileScan csv [DEST_COUNTRY_NAME#168] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/databricks-datasets/definitive-guide/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string&gt;\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["# Execute a command in the DataFrame way\ndataFrameWay = flightData2015\\\n  .groupBy(\"DEST_COUNTRY_NAME\")\\\n  .count()\ndataFrameWay.explain()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\n*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#168], functions=[finalmerge_count(merge count#197L) AS count(1)#192L])\n+- Exchange hashpartitioning(DEST_COUNTRY_NAME#168, 200)\n   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#168], functions=[partial_count(1) AS count#197L])\n      +- *(1) FileScan csv [DEST_COUNTRY_NAME#168] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/databricks-datasets/definitive-guide/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string&gt;\n</div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["We get the same explain plan!"],"metadata":{}},{"cell_type":"code","source":["# Execute another action in the DataFrame\nfrom pyspark.sql.functions import max\nflightData2015.select(max(\"count\")).take(1)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">25</span><span class=\"ansired\">]: </span>[Row(max(count)=370002)]</div>"]}}],"execution_count":12},{"cell_type":"code","source":["# Execute a more complex command in the sql way\nmaxSql = spark.sql(\"\"\"\nSELECT DEST_COUNTRY_NAME, sum(count) as destination_total\nFROM flight_data_2015\nGROUP BY DEST_COUNTRY_NAME\nORDER BY sum(count) DESC\nLIMIT 5\n\"\"\")\n\nmaxSql.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-----------------+\nDEST_COUNTRY_NAME|destination_total|\n+-----------------+-----------------+\n    United States|           411352|\n           Canada|             8399|\n           Mexico|             7140|\n   United Kingdom|             2025|\n            Japan|             1548|\n+-----------------+-----------------+\n\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["# Execute it in the DataFrame way\nfrom pyspark.sql.functions import desc\n\nflightData2015\\\n  .groupBy(\"DEST_COUNTRY_NAME\")\\\n  .sum(\"count\")\\\n  .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n  .sort(desc(\"destination_total\"))\\\n  .limit(5)\\\n  .show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-----------------+\nDEST_COUNTRY_NAME|destination_total|\n+-----------------+-----------------+\n    United States|           411352|\n           Canada|             8399|\n           Mexico|             7140|\n   United Kingdom|             2025|\n            Japan|             1548|\n+-----------------+-----------------+\n\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["# And check the new explain plan\nflightData2015\\\n  .groupBy(\"DEST_COUNTRY_NAME\")\\\n  .sum(\"count\")\\\n  .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n  .sort(desc(\"destination_total\"))\\\n  .limit(5)\\\n  .explain()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\nTakeOrderedAndProject(limit=5, orderBy=[destination_total#292L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#168,destination_total#292L])\n+- *(2) HashAggregate(keys=[DEST_COUNTRY_NAME#168], functions=[finalmerge_sum(merge sum#296L) AS sum(cast(count#170 as bigint))#288L])\n   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#168, 200)\n      +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#168], functions=[partial_sum(cast(count#170 as bigint)) AS sum#296L])\n         +- *(1) FileScan csv [DEST_COUNTRY_NAME#168,count#170] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/databricks-datasets/definitive-guide/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,count:int&gt;\n</div>"]}}],"execution_count":15}],"metadata":{"name":"Chapter_2_A_Gentle_Introduction_to_Spark","notebookId":3728306823410824},"nbformat":4,"nbformat_minor":0}
