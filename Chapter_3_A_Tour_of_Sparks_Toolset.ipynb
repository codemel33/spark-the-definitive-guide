{"cells":[{"cell_type":"markdown","source":["# Structured Streaming"],"metadata":{}},{"cell_type":"code","source":["# Load the data\nstaticDataFrame = spark.read.format(\"csv\")\\\n  .option(\"header\", \"true\")\\\n  .option(\"inferSchema\", \"true\")\\\n  .load(\"/databricks-datasets/definitive-guide/data/retail-data/by-day/*.csv\")\n\nstaticDataFrame.createOrReplaceTempView(\"retail_data\")\nstaticSchema = staticDataFrame.schema"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["# Create the daily spends column\nfrom pyspark.sql.functions import window, column, desc, col\n\nstaticDataFrame\\\n  .selectExpr(\n    \"CustomerId\",\n    \"(UnitPrice * Quantity) as total_cost\",\n    \"InvoiceDate\")\\\n  .groupBy(\n    col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n  .sum(\"total_cost\")\\\n  .sort(desc(\"sum(total_cost)\"))\\\n  .show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+--------------------+------------------+\nCustomerId|              window|   sum(total_cost)|\n+----------+--------------------+------------------+\n   17450.0|[2011-09-20 00:00...|          71601.44|\n      null|[2011-11-14 00:00...|          55316.08|\n      null|[2011-11-07 00:00...|          42939.17|\n      null|[2011-03-29 00:00...| 33521.39999999998|\n      null|[2011-12-08 00:00...|31975.590000000007|\n+----------+--------------------+------------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["# Reduce the number of partitions since there are not too many workers\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n\n# Read the data in Stream\nstreamingDataFrame = spark.readStream\\\n    .schema(staticSchema)\\\n    .option(\"maxFilesPerTrigger\", 1)\\\n    .format(\"csv\")\\\n    .option(\"header\", \"true\")\\\n    .load(\"/data/retail-data/by-day/*.csv\")\n\n# Check the streaming\nstreamingDataFrame.isStreaming"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">16</span><span class=\"ansired\">]: </span>True</div>"]}}],"execution_count":4},{"cell_type":"code","source":["# Create again the daily spends column\npurchaseByCustomerPerHour = streamingDataFrame\\\n  .selectExpr(\n    \"CustomerId\",\n    \"(UnitPrice * Quantity) as total_cost\",\n    \"InvoiceDate\")\\\n  .groupBy(\n    col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n  .sum(\"total_cost\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["# Perform and action\npurchaseByCustomerPerHour.writeStream\\\n    .format(\"memory\")\\\n    .queryName(\"customer_purchases\")\\\n    .outputMode(\"complete\")\\\n    .start()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">18</span><span class=\"ansired\">]: </span>&lt;pyspark.sql.streaming.StreamingQuery at 0x7f88feb0a470&gt;</div>"]}}],"execution_count":6},{"cell_type":"code","source":["spark.sql(\"\"\"\n  SELECT *\n  FROM customer_purchases\n  ORDER BY `sum(total_cost)` DESC\n  \"\"\")\\\n  .show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+------+---------------+\nCustomerId|window|sum(total_cost)|\n+----------+------+---------------+\n+----------+------+---------------+\n\n</div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["# Machine Learning and Advanced Analytics"],"metadata":{}},{"cell_type":"code","source":["# Work out the missing values\nfrom pyspark.sql.functions import date_format, col\n\npreppedDataFrame = staticDataFrame\\\n  .na.fill(0)\\\n  .withColumn(\"day_of_week\", date_format(col(\"InvoiceDate\"), \"EEEE\"))\\\n  .coalesce(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["# Create a train and a test set\ntrainDataFrame = preppedDataFrame\\\n  .where(\"InvoiceDate < '2011-07-01'\")\ntestDataFrame = preppedDataFrame\\\n  .where(\"InvoiceDate >= '2011-07-01'\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["# Give days numerical values\nfrom pyspark.ml.feature import StringIndexer\n\nindexer = StringIndexer()\\\n  .setInputCol(\"day_of_week\")\\\n  .setOutputCol(\"day_of_week_index\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"code","source":["# Break the order by setting a categorical variable\nfrom pyspark.ml.feature import OneHotEncoder\n\nencoder = OneHotEncoder()\\\n  .setInputCol(\"day_of_week_index\")\\\n  .setOutputCol(\"day_of_week_encoded\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"code","source":["# Assemble all features into a vector\nfrom pyspark.ml.feature import VectorAssembler\n\nvectorAssembler = VectorAssembler()\\\n  .setInputCols([\"UnitPrice\", \"Quantity\", \"day_of_week_encoded\"])\\\n  .setOutputCol(\"features\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"code","source":["# Set up a pipeline\nfrom pyspark.ml import Pipeline\n\ntransformationPipeline = Pipeline()\\\n  .setStages([indexer, encoder, vectorAssembler])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":14},{"cell_type":"code","source":["# Fit and transform the data through the pipeline\nfittedPipeline = transformationPipeline.fit(trainDataFrame)\ntransformedTraining = fittedPipeline.transform(trainDataFrame)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":15},{"cell_type":"code","source":["# Cache the DataFrame to improve times\ntransformedTraining.cache()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">27</span><span class=\"ansired\">]: </span>DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string, day_of_week: string, day_of_week_index: double, day_of_week_encoded: vector, features: vector]</div>"]}}],"execution_count":16},{"cell_type":"code","source":["# Instantiate the model and make it work\nfrom pyspark.ml.clustering import KMeans\nkmeans = KMeans()\\\n  .setK(20)\\\n  .setSeed(1)\n\nkmModel = kmeans.fit(transformedTraining)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17},{"cell_type":"code","source":["# Use the model to make predictions\n\ntransformedTest = fittedPipeline.transform(testDataFrame)\nkmModel.computeCost(transformedTest)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">38</span><span class=\"ansired\">]: </span>517507094.72221166</div>"]}}],"execution_count":18},{"cell_type":"markdown","source":["# Lower-Level APIs"],"metadata":{}},{"cell_type":"code","source":["# Transform a RDD into a DataFrame\nfrom pyspark.sql import Row\n\nspark.sparkContext.parallelize([Row(1), Row(2), Row(3)]).toDF()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">39</span><span class=\"ansired\">]: </span>DataFrame[_1: bigint]</div>"]}}],"execution_count":20}],"metadata":{"name":"Chapter_3_A_Tour_of_Sparks_Toolset","notebookId":3728306823410846},"nbformat":4,"nbformat_minor":0}
